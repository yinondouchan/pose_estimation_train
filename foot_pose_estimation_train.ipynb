{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28279,
     "status": "ok",
     "timestamp": 1579605964805,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "SK_fpswLQjg8",
    "outputId": "dcfec17e-94ae-436e-f46f-674efa351209"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1579605999273,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "Tl0-1mWcm6k5",
    "outputId": "eed6472f-f626-4571-cf62-f53d5a31a72e"
   },
   "outputs": [],
   "source": [
    "%cd drive/My Drive/colab_notebooks/pose_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkHRtQpQgifZ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4I6Ea1agj6d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import torch\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import OrderedDict\n",
    "from generate_cmap_paf import generate_cmap, generate_paf, annotations_to_connections, annotations_to_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhnDVsmHCkjS"
   },
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81GwFwKHCjMu"
   },
   "outputs": [],
   "source": [
    "from pose_datasets import FootOnlyConfig, BodyAndFeetConfig, BodyOnlyConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vMCQBZgYxGd"
   },
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-w3HyBFgCz-J"
   },
   "outputs": [],
   "source": [
    "from pose_datasets import FootPoseDataset, HandPoseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YUX4bmmQVGV"
   },
   "source": [
    "## Transform for adding gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BC25S7gYX4Up"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return torch.clamp(tensor + torch.randn(tensor.size()) * self.std + self.mean, 0, 1)\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSYLzy7fH_-k"
   },
   "outputs": [],
   "source": [
    "def init_model_to_transfer_learning(pretrained_model, num_parts, num_links, num_upsample=3):\n",
    "    \"\"\"\n",
    "    1. Disable backpropagation for the backbone\n",
    "    2. Initialze the pretrained model by leaving the backbone's weights unchanged\n",
    "    and resetting weights and output shape of the the paf and cmap extractors\n",
    "    \"\"\"\n",
    "\n",
    "    # disable backpropagation for the backbone\n",
    "    # for param in pretrained_model[0].parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # # # disable batch normalization for the backbone\n",
    "    # for m in pretrained_model[0].modules():\n",
    "    #     if isinstance(m, torch.nn.BatchNorm2d):\n",
    "    #         m.eval()\n",
    "\n",
    "    feature_channels = 512\n",
    "    new_cmap_channels = num_parts\n",
    "    new_paf_channels = 2 * num_links\n",
    "    upsample_channels = 256\n",
    "    num_flat = 0\n",
    "\n",
    "    # initialize new cmap and paf extractor\n",
    "    new_cmap_paf_layer = CmapPafHeadAttention(feature_channels, new_cmap_channels, new_paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n",
    "\n",
    "    # put the new extractor in the model instead of the old pretrained one\n",
    "    pretrained_model[1] = new_cmap_paf_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOeAVbeqJNt-"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5k8CjciREE6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_only=False, metric=None, scheduler=None, stdev_scheduler=None, window_scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('Inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        phases = ['train'] if train_only else ['train', 'val']\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            # schedule heatmap standard deviation\n",
    "            if stdev_scheduler is not None:\n",
    "                new_stdev = stdev_scheduler(epoch)\n",
    "                print('Setting %s dataset stdev to %s' % (phase, new_stdev))\n",
    "                dataloaders[phase].dataset.set_stdev(new_stdev)\n",
    "\n",
    "                # if new_stdev <= 1.0:\n",
    "                #     print('Setting kernel type to pinpoint')\n",
    "                #     dataloaders[phase].dataset.set_kernel_type('pinpoint')\n",
    "\n",
    "            # schedule heatmap window size\n",
    "            if window_scheduler is not None:\n",
    "                new_window = window_scheduler(epoch)\n",
    "                print('Setting %s dataset window to %s' % (phase, new_window))\n",
    "                dataloaders[phase].dataset.set_window(new_window)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            sample_count = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, cmaps, pafs in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                cmaps = cmaps.to(device)\n",
    "                pafs = pafs.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    cmap_loss = criterion(outputs[0], cmaps)\n",
    "                    paf_loss = criterion(outputs[1], pafs)\n",
    "                    cmap_metric = metric(outputs[0], cmaps)\n",
    "                    paf_metric = metric(outputs[1], pafs)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        cmap_loss.backward(retain_graph=True)\n",
    "                        paf_loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    sample_count += inputs.size(0)\n",
    "                    if sample_count % (inputs.size(0) * 4) == 0:\n",
    "                        status_line = '{}, after {} samples, batch cmap loss: {:.6f}, batch paf loss: {:.6f}'.format(phase, sample_count, cmap_loss, paf_loss)\n",
    "                        if metric is not None:\n",
    "                            status_line += ', batch cmap metric: {:.6f}, batch paf metric: {:.6f}'.format(cmap_metric, paf_metric)\n",
    "\n",
    "                        print(status_line)\n",
    "\n",
    "\n",
    "                # measure running loss as the average between cmap and paf loss\n",
    "                running_loss += cmap_loss.item() + paf_loss.item() * inputs.size(0) / 2.0\n",
    "\n",
    "            if scheduler is not None and phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / sample_count\n",
    "\n",
    "            print('{} Loss: {:.6f}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            eval_phase = 'train' if train_only else 'val'\n",
    "            if phase == eval_phase and epoch_loss < best_loss:\n",
    "                print('Updating best model weights')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:6f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTymNhFSesp8"
   },
   "outputs": [],
   "source": [
    "def sum_of_squared_errors(output, target):\n",
    "    loss = torch.sum((output - target)**2) / output.shape[0]\n",
    "    return loss\n",
    "\n",
    "def mse_punish_false_negative(output, target):\n",
    "    # same as MSE but with false negatives punished harder\n",
    "    weights = torch.ones_like(target)\n",
    "    weights[(target >= 0.1)] = 3\n",
    "\n",
    "    return torch.mean(weights * (output - target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPgNkPjRnbCz"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-w8d4u1Dcpo"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_data_path = 'datasets/body_and_foot_pose/person_keypoints_train2017_foot_v1.json'\n",
    "val_data_path = 'datasets/body_and_foot_pose/person_keypoints_val2017_foot_v1.json'\n",
    "\n",
    "with open(train_data_path) as json_file:\n",
    "    train_keypoints = json.load(json_file)\n",
    "with open(val_data_path) as json_file:\n",
    "    val_keypoints = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzRULzdSaxia"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/body_and_foot_pose/train/train.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b03db7d649f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                            \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                                            \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                                            ]), source='zip', use_cache=False)\n\u001b[0m\u001b[1;32m     11\u001b[0m val_dataset = FootPoseDataset(train_data_path, mode='train', config=dataset_config, transform=transforms.Compose([\n\u001b[1;32m     12\u001b[0m                                                                         \u001b[0;31m#    transforms.Resize((224, 224)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/pose_estimation/train_code/pose_datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, config, transform, mode, source, device, use_cache, cmap_kernel_type)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mzf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body_and_foot_pose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# preload images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/body_and_foot_pose/train/train.zip'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "dataset_config = BodyAndFeetConfig(output_width=56, output_height=56)\n",
    "\n",
    "train_dataset = FootPoseDataset(train_data_path, mode='train', config=dataset_config, transform=transforms.Compose([\n",
    "                                                                        #    transforms.Resize((224, 224)),\n",
    "                                                                           transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "                                                                           transforms.ToTensor(),\n",
    "                                                                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                                                           ]), source='zip', use_cache=False)\n",
    "val_dataset = FootPoseDataset(train_data_path, mode='train', config=dataset_config, transform=transforms.Compose([\n",
    "                                                                        #    transforms.Resize((224, 224)),\n",
    "                                                                           transforms.ToTensor(),\n",
    "                                                                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                                                           ]), source='zip', use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7527,
     "status": "ok",
     "timestamp": 1579606057500,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "hsjc_QVv9uWv",
    "outputId": "378265dd-d218-47c6-a50a-cd52832cc599"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from generate_cmap_paf import generate_cmap, generate_paf, annotations_to_connections, annotations_to_peaks\n",
    "\n",
    "train_dataset.set_stdev(1)\n",
    "#train_dataset.set_kernel_type('pinpoint')\n",
    "#train_dataset.set_window(9.0)\n",
    "\n",
    "ind = 19\n",
    "img_raw, ann = train_dataset.get_image_and_annotation_data(ind)\n",
    "img, cmap, paf = train_dataset[ind]\n",
    "pil_tf = transforms.ToPILImage()\n",
    "plt.imshow(pil_tf(img))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(torch.clamp(torch.sum(cmap, axis=0), 0, 1))\n",
    "plt.show()\n",
    "plt.imshow(torch.clamp(torch.sum(paf, axis=0), -1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvkfDt95oal8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "nsamples = len(train_dataset)\n",
    "train_val_split = nsamples - 500\n",
    "\n",
    "sample_permutation = torch.randperm(nsamples)\n",
    "train_range = list(range(train_val_split))\n",
    "val_range = list(range(train_val_split, nsamples))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(sample_permutation[train_range])\n",
    "val_sampler = SubsetRandomSampler(sample_permutation[val_range])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': data.DataLoader(train_dataset, batch_size=32, num_workers=4, sampler=train_sampler),\n",
    "    'val': data.DataLoader(val_dataset, batch_size=32, num_workers=4, sampler=val_sampler)\n",
    "}\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1579389420225,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "dubn_fxxA6Mv",
    "outputId": "6ded42e5-cfb5-459f-8dce-2f99ab552d58"
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from resnet import resnet18_baseline_att\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# load the pre-trained model for COCO annotations\n",
    "model = resnet18_baseline_att(18, 2 * 21).to(device)\n",
    "model_weights_path = 'resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "model.load_state_dict(torch.load(model_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMZ4gw4DOx5o"
   },
   "outputs": [],
   "source": [
    "init_model_to_transfer_learning(model, train_dataset.config.num_parts, train_dataset.config.num_links, num_upsample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNa0EOGDT6O2"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "metric = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6116094,
     "status": "ok",
     "timestamp": 1579395539905,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "0mM9kSirzQGl",
    "outputId": "65ac56cb-e0ec-4841-8494-b4296ea3d352"
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "stdev_scheduler = lambda ep: max((20 - ep) / 4 + 1.0, 1.0)\n",
    "model = train_model(model.to(device), criterion, optimizer, metric=metric, num_epochs=num_epochs, train_only=False,\n",
    "                    scheduler=None, stdev_scheduler=stdev_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwO2WtaF_kVb"
   },
   "outputs": [],
   "source": [
    "train_dataset_unnormed = FootPoseDataset(train_data_path, mode='train', config=dataset_config, transform=transforms.Compose([\n",
    "                                                                           transforms.Resize((224, 224)),\n",
    "                                                                           transforms.ToTensor()\n",
    "                                                                           ]), source='zip')\n",
    "val_dataset_unnormed = FootPoseDataset(val_data_path, mode='val', config=dataset_config, transform=transforms.Compose([\n",
    "                                                                           transforms.Resize((224, 224)),\n",
    "                                                                           transforms.ToTensor()\n",
    "                                                                           ]), source='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1892,
     "status": "ok",
     "timestamp": 1579403808899,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "dJ52SaVltgNx",
    "outputId": "a88be20f-de91-48b4-91be-1580a46fb87e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind = 0\n",
    "img_ind = 19\n",
    "# img_mean_inv = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n",
    "# img_std_inv = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n",
    "# img_if = img * img_std_inv ** 2 + img_mean_inv\n",
    "img_orig, cmap, paf = train_dataset_unnormed[img_ind]\n",
    "tf = transforms.ToPILImage()\n",
    "plt.imshow(tf(img_orig))\n",
    "plt.show()\n",
    "\n",
    "print(cmap.shape)\n",
    "\n",
    "img, cmap, paf = train_dataset[img_ind]\n",
    "plt.imshow(torch.sum(cmap, axis=0), vmin=0, vmax=1)\n",
    "plt.show()\n",
    "\n",
    "cmap_inf, paf_inf = model(img.reshape(1, *img.shape).to(device))\n",
    "cmap_inf = cmap_inf.detach().cpu()\n",
    "paf_inf = paf_inf.detach().cpu()\n",
    "plt.imshow(torch.sum(cmap_inf[0], axis=0), vmin=0, vmax=1.0)\n",
    "# plt.imshow(cmap_inf[0, 5], vmin=0, vmax=1.0)\n",
    "plt.show()\n",
    "#cmap_ae = torch.sum(torch.abs(cmap_inf[0, ind] - cmap[ind]))\n",
    "#print(torch.max(cmap[ind]))\n",
    "#print(torch.max(cmap_inf[0][ind]))\n",
    "#print(cmap_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZ956JmBP9lT"
   },
   "outputs": [],
   "source": [
    "model = train_model(model.to(device), criterion, optimizer, metric=metric, num_epochs=5, train_only=False,\n",
    "                    scheduler=None, stdev_scheduler=lambda ep: 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FoPBDuBcAzNz"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'body_only_25ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1577798785852,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "CUrQCSzNkTJQ",
    "outputId": "e51f8f2a-ed59-4048-f5c2-280bfa861132"
   },
   "outputs": [],
   "source": [
    "model_orig = resnet18_baseline_att(18, 2 * 21).to(device)\n",
    "model_weights_path = 'resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "model_orig.load_state_dict(torch.load(model_weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8376,
     "status": "ok",
     "timestamp": 1577799228567,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "HPIviroUISDC",
    "outputId": "066ff6a6-0020-4e3b-e838-2512f6690c05"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind = 0\n",
    "img_ind = 14\n",
    "\n",
    "img_orig, cmap, paf = val_dataset_unnormed[img_ind]\n",
    "tf = transforms.ToPILImage()\n",
    "plt.imshow(tf(img_orig))\n",
    "plt.show()\n",
    "\n",
    "img, cmap, paf = val_dataset[img_ind]\n",
    "cmap_inf, paf_inf = model_orig(img.reshape(1, *img.shape).to(device))\n",
    "\n",
    "# for i in range(len(cmap_inf[0])):\n",
    "#     cmap_inf = cmap_inf.detach().cpu()\n",
    "#     plt.imshow(cmap_inf[0, i], vmin=0, vmax=1.0)\n",
    "#     plt.show()\n",
    "\n",
    "for i in range(len(paf_inf[0])):\n",
    "    paf_inf = paf_inf.detach().cpu()\n",
    "    plt.imshow(paf_inf[0, i], vmin=0, vmax=1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1577896905708,
     "user": {
      "displayName": "Yinon Douchan",
      "photoUrl": "",
      "userId": "12948713182025833561"
     },
     "user_tz": -120
    },
    "id": "rt8Nwq58JhTI",
    "outputId": "c4a74564-c46f-4828-e436-bd7414214f65"
   },
   "outputs": [],
   "source": [
    "train_keypoints['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdhnasQltkgf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "foot_pose_estimation_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
